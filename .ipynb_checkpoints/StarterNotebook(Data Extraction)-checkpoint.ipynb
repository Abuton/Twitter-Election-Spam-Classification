{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a231513",
   "metadata": {},
   "source": [
    "**Question 1** : <h3> Use the following function as is for initial cleaning of the tweet data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb94d42",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/abubakar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/abubakar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import preprocessor as p\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "def clean_tweets(twitter_text:list)->list:\n",
    "    \"\"\"\n",
    "    clean_tweets helps remove unwanted text,emoticons etc from tweets\n",
    "    it is the first function you run after getting the data\n",
    "    Args:\n",
    "    ----\n",
    "    twitter_text: list\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    a list of json\n",
    "    \n",
    "    \"\"\"\n",
    "    #use pre processor\n",
    "    tweet = p.clean(twitter_text)\n",
    "\n",
    "     #HappyEmoticons\n",
    "    emoticons_happy = set([\n",
    "        ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "        ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "        '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "        'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "        '<3'\n",
    "        ])\n",
    "\n",
    "    # Sad Emoticons\n",
    "    emoticons_sad = set([\n",
    "        ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "        ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "        ':c', ':{', '>:\\\\', ';('\n",
    "        ])\n",
    "\n",
    "    #Emoji patterns\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "             u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "             u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "             u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "             u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "             u\"\\U00002702-\\U000027B0\"\n",
    "             u\"\\U000024C2-\\U0001F251\"\n",
    "             \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    #combine sad and happy emoticons\n",
    "    emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = nltk.word_tokenize(tweet)\n",
    "    #after tweepy preprocessing the colon symbol left remain after      \n",
    "    #removing mentions\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "\n",
    "    #remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "    #filter using NLTK library append it to a string\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "    #looping through conditions\n",
    "    filtered_tweet = []    \n",
    "    for w in word_tokens:\n",
    "    #check tokens against stop words , emoticons and punctuations\n",
    "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "            filtered_tweet.append(w)\n",
    "\n",
    "    return ' '.join(filtered_tweet)            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155d837",
   "metadata": {},
   "source": [
    "**Question 2** : <h3> Complete the `Tweet_df` class with functions implementing all the columns that has been specified</h3> An Example implementation is provided as well\n",
    "columns = ```'created_at', 'source', 'original_text','clean_text', \n",
    "                    'sentiment','polarity','subjectivity', 'lang',\n",
    "                    'favorite_count', 'retweet_count', 'original_author', 'screen_count',\n",
    "                    'followers_count','friends_count','possibly_sensitive', 'hashtags',\n",
    "                    'user_mentions', 'place', 'place_coord_boundaries'```\n",
    "\n",
    "<h5>Bonus : Explore other features/attributes present in the json file that has not been listed here and write functions to extract those features/attribbutes</h5>\n",
    "\n",
    "NB: The `Tweet_df` class is suppose to return a dataframe that has all the listed columns above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b537c06e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-94dca5109e85>, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-94dca5109e85>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    class Tweet_df(self, csv_file:str, json_file:list, colunms=cols)->pd.DataFrame:\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def read_json(json_file: str)->list:\n",
    "    \"\"\"\n",
    "    json file reader to open and read json files into a list\n",
    "    Args:\n",
    "    -----\n",
    "    json_file: str - path of a json file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    length of the json file and a list of json\n",
    "    \"\"\"\n",
    "    \n",
    "    tweets_data = []\n",
    "    for tweets in open(json_file,'r'):\n",
    "        tweets_data.append(json.loads(tweets))\n",
    "    \n",
    "    \n",
    "    return len(tweets_data), tweets_data\n",
    "\n",
    "# required column to be generated you should be creative and add more features\n",
    "columns = ['created_at', 'source', 'original_text','clean_text', \n",
    "                    'sentiment','polarity','subjectivity', 'lang',\n",
    "                    'favorite_count', 'retweet_count', 'original_author', 'screen_count',\n",
    "                    'followers_count','friends_count','possibly_sensitive', 'hashtags',\n",
    "                    'user_mentions', 'place', 'place_coord_boundaries']\n",
    "            \n",
    "class Tweet_df:\n",
    "    \"\"\"\n",
    "    this function will parse tweets json into a pandas dataframe\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    csv_file: name for the output dataframe\n",
    "    json_file: a list of jsons that contains tweets\n",
    "    columns: expected column name\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    dataframe\n",
    "    \"\"\"\n",
    "    def __init__(self, data, columns):\n",
    "        \n",
    "        self.data = data\n",
    "        self.columns = columns\n",
    "\n",
    "        \n",
    "    def find_statuses_count(self, tweets_list:list)->list:\n",
    "        statuses_count = [x['user']['statuses_count'] for x in tweets_list]\n",
    "        return statuses_count\n",
    "        \n",
    "        ------\n",
    "        -------\n",
    "        \n",
    "        ------\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30172612",
   "metadata": {},
   "source": [
    "**Question 3** <h3>Complete the Skeleton class `Clean_Tweets` by implementing all the listed functions</h3>\n",
    "<h4>Use the completed class to further preprocess the extracted twitter dataframe</h4>\n",
    "\n",
    "Bonus: Explore the data after final cleaning and report \n",
    "1. The Most retweeted Tweet\n",
    "2. The most popular twitter user popularity = followers_count + friends_count\n",
    "3. Top 5 hashtags\n",
    "4. Top 10 Most mentioned Users\n",
    "5. Top 5 users based on Statuses_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c480e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clean_Tweets:\n",
    "    \"\"\"\n",
    "    The PEP8 Standard AMAZING!!!\n",
    "    \"\"\"\n",
    "    def __init__(self, df:pd.DataFrame):\n",
    "        self.df = df\n",
    "        print('Automation in Action...!!!')\n",
    "        \n",
    "    def drop_unwanted_column(self, df:pd.DataFrame)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        remove rows that has column names. This error originated from\n",
    "        the data collection stage.  \n",
    "        \"\"\"\n",
    "        unwanted_rows = df[df['retweet_count'] == 'retweet_count' ].index\n",
    "        df.drop(unwanted_rows , inplace=True)\n",
    "        df = df[df['polarity'] != 'polarity']\n",
    "        \n",
    "        return df\n",
    "    def drop_duplicate(self, df:pd.DataFrame)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        drop duplicate rows\n",
    "        \"\"\"\n",
    "        \n",
    "        ---\n",
    "        \n",
    "        return df\n",
    "    def convert_to_datetime(self, df:pd.DataFrame)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        convert column to datetime\n",
    "        \"\"\"\n",
    "        ----\n",
    "        \n",
    "        ----\n",
    "        \n",
    "        df = df[df['created_at'] >= '2020-12-31' ]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def convert_to_numbers(self, df:pd.DataFrame)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        convert columns like polarity, subjectivity, retweet_count\n",
    "        favorite_count etc to numbers\n",
    "        \"\"\"\n",
    "        df['polarity'] = pd.----\n",
    "        \n",
    "        ----\n",
    "        ----\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def remove_non_english_tweets(self, df:pd.DataFrame)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        remove non english tweets from lang\n",
    "        \"\"\"\n",
    "        \n",
    "        df = ----\n",
    "        \n",
    "        return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
