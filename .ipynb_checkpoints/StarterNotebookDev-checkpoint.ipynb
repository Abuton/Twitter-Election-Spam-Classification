{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c0cd17",
   "metadata": {},
   "source": [
    "https://rpubs.com/JDAHAN/172473\n",
    "\n",
    "http://datascientist.one/questions-answers-machine-learning-math/\n",
    "\n",
    "https://www.dataquest.io/blog/streaming-data-python/\n",
    "\n",
    "https://www.youtube.com/watch?v=kTq_GY50Ek8\n",
    "\n",
    "https://medium.com/analytics-vidhya/automating-python-unit-testing-using-travis-ci-be4fc965c1c0\n",
    "\n",
    "https://drive.google.com/drive/u/1/folders/1xDKlP-vCapaKA__x0eoihZhnGQ-Cpe14\n",
    "\n",
    "https://www.stitchdata.com/resources/what-is-data-pipeline/\n",
    "\n",
    "https://docs.google.com/forms/d/1ugQrkwCe_vr6iqUxwFmK7YJCxUIiM8r3d5fFHs-E8_8/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48553f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "29f85062",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/abubakar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/abubakar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import preprocessor as p\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "def clean_tweets(twitter_text:list)->list:\n",
    "    \"\"\"\n",
    "    clean_tweets helps remove unwanted text,emoticons etc from tweets\n",
    "    it is the first function you run after getting the data\n",
    "    Args:\n",
    "    ----\n",
    "    twitter_text: list\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    a list of json\n",
    "    \n",
    "    \"\"\"\n",
    "    #use pre processor\n",
    "    tweet = p.clean(twitter_text)\n",
    "\n",
    "     #HappyEmoticons\n",
    "    emoticons_happy = set([\n",
    "        ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "        ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "        '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "        'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "        '<3'\n",
    "        ])\n",
    "\n",
    "    # Sad Emoticons\n",
    "    emoticons_sad = set([\n",
    "        ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "        ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "        ':c', ':{', '>:\\\\', ';('\n",
    "        ])\n",
    "\n",
    "    #Emoji patterns\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "             u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "             u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "             u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "             u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "             u\"\\U00002702-\\U000027B0\"\n",
    "             u\"\\U000024C2-\\U0001F251\"\n",
    "             \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    #combine sad and happy emoticons\n",
    "    emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = nltk.word_tokenize(tweet)\n",
    "    #after tweepy preprocessing the colon symbol left remain after      \n",
    "    #removing mentions\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r'â€šÃ„Â¶', '', tweet)\n",
    "\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "\n",
    "    #remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "    #filter using NLTK library append it to a string\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "    #looping through conditions\n",
    "    filtered_tweet = []    \n",
    "    for w in word_tokens:\n",
    "    #check tokens against stop words , emoticons and punctuations\n",
    "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "            filtered_tweet.append(w)\n",
    "\n",
    "    return ' '.join(filtered_tweet)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "73fa7fdd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json  \n",
    "def read_json(json_file: str):\n",
    "    \"\"\"\n",
    "    json file reader\n",
    "    Args:\n",
    "    -----\n",
    "    json_file: str - path of a json file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    length of the json file and a list of json\n",
    "    \"\"\"\n",
    "    \n",
    "    tweets_data = []\n",
    "    for tweets in open(json_file,'r'):\n",
    "        tweets_data.append(json.loads(tweets))\n",
    "    \n",
    "    \n",
    "    return len(tweets_data), tweets_data\n",
    "\n",
    "# required column to be generated you should be creative and add more features\n",
    "cols = ['created_at', 'source', 'original_text','clean_text', \n",
    "                    'sentiment','polarity','subjectivity', 'lang',\n",
    "                    'favorite_count', 'retweet_count', 'original_author', 'screen_count',\n",
    "                    'followers_count','friends_count','possibly_sensitive', 'hashtags',\n",
    "                    'user_mentions', 'place', 'place_coord_boundaries']\n",
    "            \n",
    "# def get_tweets_df(csv_file:str, json_file:list, colunms=cols)->pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     this function will parse tweets json into a pandas dataframe\n",
    "    \n",
    "#     Args\n",
    "#     ----\n",
    "#     csv_file: name for the output dataframe\n",
    "#     json_file: a list of jsons that contains tweets\n",
    "#     columns: expected column name\n",
    "    \n",
    "#     Return\n",
    "#     ------\n",
    "#     dataframe\n",
    "#     \"\"\"\n",
    "    \n",
    "#     df = pd.DataFrame(columns=cols)\n",
    "\n",
    "#     for tweets in json_file:\n",
    "#         if tweets == \" \":\n",
    "#             continue\n",
    "            \n",
    "#         new_entry = []\n",
    "        \n",
    "#         #if this tweet is a retweet update retweet count\n",
    "#         if tweets['created_at'] in df['created_at'].values:\n",
    "#             i = df.loc[df['created_at'] == tweets['created_at']].index[0]\n",
    "#             #\n",
    "#             cond1 = tweets['favorite_count'] != df.at[i, 'favorite_count']\n",
    "#             cond2 = tweets['retweet_count'] != df.at[i, 'retweet_count']\n",
    "#             if cond1 or cond2:\n",
    "#                 df.at[i, 'favorite_count'] = tweets['favorite_count']\n",
    "#                 df.at[i, 'retweet_count'] = tweets['retweet_count']\n",
    "#             continue\n",
    "\n",
    "#         #calculate sentiment\n",
    "#         try:\n",
    "#             filtered_tweet = clean_tweets(tweets[---]['extended_tweet']['full_text']) # fix missing\n",
    "#             blob = TextBlob(filtered_tweet)\n",
    "#             Sentiment = blob.sentiment     \n",
    "#             polarity = Sentiment.polarity\n",
    "#             subjectivity = Sentiment.subjectivity\n",
    "#         except KeyError:\n",
    "#             filtered_tweet, blob, Sentiment, polarity, subjectivity = 0,0,0,0,0\n",
    "#         try:\n",
    "#             new_entry += [tweets['created_at'],\n",
    "#                           tweets['source'], tweets['retweeted_status']['extended_tweet']['full_text'], filtered_tweet, \n",
    "#                           Sentiment,polarity,subjectivity, tweets['lang'],\n",
    "#                           tweets['favorite_count'], tweets['retweet_count']]\n",
    "\n",
    "#             new_entry.append(tweets['user']['screen_name'])\n",
    "#             new_entry.append(tweets['user']['statuses_count'])\n",
    "#             new_entry.append(tweets['user']['followers_count'])\n",
    "#             new_entry.append(tweets['user']['friends_count'])\n",
    "#         except Exception as e:\n",
    "#             continue\n",
    "              \n",
    "#         try:\n",
    "#             is_sensitive = tweets['possibly_sensitive']\n",
    "#         except KeyError:\n",
    "#             is_sensitive = None\n",
    "\n",
    "#         new_entry.append(is_sensitive)\n",
    "        \n",
    "#         # fix hashtags such that you have 'words' instead of [words]\n",
    "#         hashtags = [hashtag_item['text'] for hashtag_item in tweets['entities']['hashtags']]\n",
    "#         new_entry.append(hashtags) #append the hashtags\n",
    "\n",
    "#         # fix mention such that you have 'user mention' instead of [user mention]\n",
    "#         mentions = mention['screen_name'] for mention in tweets['entities']['user_mentions']\n",
    "#         new_entry.append(mentions) #append the user mentions\n",
    "\n",
    "#         try:\n",
    "#             xyz = tweets['place']['bounding_box']['coordinates']\n",
    "#             coordinates = [coord for loc in xyz for coord in loc]\n",
    "#         except TypeError:\n",
    "#             coordinates = None\n",
    "#         #\n",
    "#         new_entry.append(coordinates)\n",
    "\n",
    "#         try:\n",
    "#             location = tweets['user']['location']\n",
    "#         except TypeError:\n",
    "#             location = ''\n",
    "#         #\n",
    "#         new_entry.append(location)\n",
    "\n",
    "#         #now append a row to the dataframe\n",
    "#         single_tweet_df = pd.DataFrame(new_entry, columns=---) # fix code\n",
    "#         df = df.append(single_tweet_df, ignore_index=True)\n",
    "\n",
    "#         #\n",
    "#         df['timestamp'] = df.created_at.map(pd.Timestamp)\n",
    "#         df = df.sort_values('timestamp').set_index('timestamp')\n",
    "        \n",
    "#         if not csv_file is None:\n",
    "#             #save it to file\n",
    "#             df.to_csv(csv_file,mode='a', index=True, encoding=\"utf-8\")\n",
    "            \n",
    "#         return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "fbef08b0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-219-bcdf092956a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mtwitter_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMyListener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mtwitter_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist_of_keywords_fin_tech\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/workspace/.conda/lib/python3.8/site-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, follow, track, is_async, locations, stall_warnings, languages, encoding, filter_level)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filter_level'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_level\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'delimited'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'length'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_async\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     def sitestream(self, follow, stall_warnings=False,\n",
      "\u001b[0;32m/workspace/.conda/lib/python3.8/site-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36m_start\u001b[0;34m(self, is_async)\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.conda/lib/python3.8/site-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnooze_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnooze_time_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0;31m# This is still necessary, as a SSLError can actually be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.conda/lib/python3.8/site-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36m_read_loop\u001b[0;34m(self, resp)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                 \u001b[0mstripped_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mline\u001b[0m \u001b[0;31m# line is sometimes None so we need to check here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstripped_line\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.conda/lib/python3.8/site-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36mread_line\u001b[0;34m(self, sep)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.conda/lib/python3.8/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m                 if (\n\u001b[1;32m    520\u001b[0m                     \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.conda/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.conda/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readinto_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.conda/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_readinto_chunked\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m                 \u001b[0mchunk_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_chunk_left\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk_left\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.conda/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_get_chunk_left\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    549\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# toss the CRLF at the end of the chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0mchunk_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_next_chunk_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.conda/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_read_next_chunk_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_next_chunk_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;31m# Read the next chunk size from the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"chunk size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.conda/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.conda/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.conda/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "\n",
    "def get_twauth():\n",
    "    path = Path('.') / '.env/twitter_keys.json'\n",
    "    twitter_auth = json.load(open(path))\n",
    "    return twitter_auth\n",
    "\n",
    "tw_auth = get_twauth()\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "\n",
    "\n",
    "list_of_keywords_fin_tech = ['Mobile Money', 'Flutterwave', \"Paystack\", 'Mpesa', \"Carbon\", \"PettySave\", \"Kuda\", \"MobileMoney\", \"Airtime\", \"USSD\",\n",
    "                             'Mpesa Transfer', 'Paystack Credit', 'Carbon MobileMoney Transfer', 'Kuda Transfer', 'Flutterwave wallet', 'Risevest' \n",
    "                             'Mobile Money Transfer', 'Mpesa MobileMoney', 'Flutterwave Cashless', 'Etransfer Africa', 'OPay', 'Palmpay', 'OneFi', 'Owo']\n",
    "\n",
    "consumer_key = tw_auth['TWITTER_CONSUMER_KEY']\n",
    "consumer_secret = tw_auth['TWITTER_CONSUMER_SECRET']\n",
    "access_token = tw_auth['TWITTER_ACCESS_TOKEN']\n",
    "access_token_secret = tw_auth['TWITTER_ACCESS_TOKEN_SECRET']\n",
    "\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "class MyListener(StreamListener):\n",
    "    \n",
    "        \n",
    "#     def on_status(self, status):\n",
    "#         if status.retweeted_status:\n",
    "#             return\n",
    "#         if status.favorite_count is None or status.favorite_count < 10:\n",
    "#             return\n",
    "#         try:\n",
    "#             with open('fintech_file.json', 'a') as f:\n",
    "#                 f.write(status)\n",
    "#                 return True\n",
    "\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open('nn.json', 'a') as f:\n",
    "                f.write(data)\n",
    "                return True\n",
    "    \n",
    "            \n",
    "        except BaseException as e:\n",
    "            print(f\"Error on_data: {e}\")     \n",
    "        return True\n",
    "    \n",
    "    def on_error(self, status):\n",
    "         if status_code == 420:\n",
    "            return False\n",
    "    \n",
    "twitter_stream = Stream(auth, MyListener())\n",
    "twitter_stream.filter(track=list_of_keywords_fin_tech, languages = [\"en\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fbeb6530",
   "metadata": {},
   "outputs": [],
   "source": [
    " def clean_tweets_df(df):\n",
    "        # drop na in clean tweets column\n",
    "        a = df[df['retweet_count'] == 'retweet_count' ].index\n",
    "        df.drop(a , inplace=True)\n",
    "        # dropduplicates and drop duplicates in clean tweets\n",
    "        df = df.drop_duplicates().drop_duplicates(subset='clean_text')\n",
    "        # convert int column appropriately\n",
    "        df = df[df['polarity'] != 'polarity']\n",
    "        # convert the created_at column to a datetime object\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "        df['timestamp'] = df['timestamp'].dt.strftime('%Y-%m-%d')\n",
    "        df = df[df['created_at'] >= '2020-12-31' ]\n",
    "        # convert int column appropriately\n",
    "        df['polarity'] = pd.to_numeric(df['polarity'], errors='coerce')\n",
    "        df['retweet_count'] = pd.to_numeric(df['retweet_count'], errors='coerce')\n",
    "        df['favorite_count'] = pd.to_numeric(df['favorite_count'], errors='coerce')\n",
    "        \n",
    "        df = df.query(\"lang == 'en' \")\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "033dc1f5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# required column to be generated you should be creative and add more features\n",
    "cols = ['id', 'created_at', 'source', 'original_text','clean_text', \n",
    "                    'sentiment','polarity','subjectivity', 'lang',\n",
    "                    'favorite_count', 'retweet_count', 'original_author', 'screen_count',\n",
    "                    'followers_count','friends_count','possibly_sensitive', 'hashtags',\n",
    "                    'user_mentions', 'place', 'place_coord_boundaries']\n",
    "\n",
    "\n",
    "def get_tweets_df(csv_file:str, json_file:list, colunms=cols)->pd.DataFrame:\n",
    "    \n",
    "    df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    for tweets in json_file:\n",
    "        new_entry = []\n",
    "        \n",
    "        #if this tweet is a retweet update retweet count\n",
    "        if tweets['created_at'] in df['created_at'].values:\n",
    "            i = df.loc[df['created_at'] == tweets['created_at']].index[0]\n",
    "            #\n",
    "            cond1 = tweets['favorite_count'] != df.at[i, 'favorite_count']\n",
    "            cond2 = tweets['retweet_count'] != df.at[i, 'retweet_count']\n",
    "            if cond1 or cond2:\n",
    "                df.at[i, 'favorite_count'] = tweets['favorite_count']\n",
    "                df.at[i, 'retweet_count'] = tweets['retweet_count']\n",
    "            continue\n",
    "\n",
    "        #calculate sentiment\n",
    "        try:\n",
    "            filtered_tweet = clean_tweets(tweets['retweeted_status']['extended_tweet']['full_text'])\n",
    "            blob = TextBlob(filtered_tweet)\n",
    "            Sentiment = blob.sentiment     \n",
    "            polarity = Sentiment.polarity\n",
    "            subjectivity = Sentiment.subjectivity\n",
    "        except KeyError:\n",
    "            filtered_tweet, blob, Sentiment, polarity, subjectivity = 0,0,0,0,0\n",
    "        try:\n",
    "            new_entry += [tweets['id'], tweets['created_at'],\n",
    "                          tweets['source'], tweets['retweeted_status']['extended_tweet']['full_text'], filtered_tweet, \n",
    "                          Sentiment,polarity,subjectivity, tweets['lang'],\n",
    "                          tweets['favorite_count'], tweets['retweet_count']]\n",
    "\n",
    "            new_entry.append(tweets['user']['screen_name'])\n",
    "            new_entry.append(tweets['user']['statuses_count'])\n",
    "            new_entry.append(tweets['user']['followers_count'])\n",
    "            new_entry.append(tweets['user']['friends_count'])\n",
    "        except Exception as e:\n",
    "            continue\n",
    "              \n",
    "        try:\n",
    "            is_sensitive = tweets['possibly_sensitive']\n",
    "        except KeyError:\n",
    "            is_sensitive = None\n",
    "\n",
    "        new_entry.append(is_sensitive)\n",
    "\n",
    "        hashtags = \", \".join([hashtag_item['text'] for hashtag_item in tweets['entities']['hashtags']])\n",
    "        new_entry.append(hashtags) #append the hashtags\n",
    "\n",
    "        #\n",
    "        mentions = \", \".join([mention['screen_name'] for mention in tweets['entities']['user_mentions']])\n",
    "        new_entry.append(mentions) #append the user mentions\n",
    "\n",
    "        try:\n",
    "            xyz = tweets['place']['bounding_box']['coordinates']\n",
    "            coordinates = [coord for loc in xyz for coord in loc]\n",
    "        except TypeError:\n",
    "            coordinates = None\n",
    "        #\n",
    "        new_entry.append(coordinates)\n",
    "\n",
    "        try:\n",
    "            location = tweets['user']['location']\n",
    "        except TypeError:\n",
    "            location = ''\n",
    "        #\n",
    "        new_entry.append(location)\n",
    "\n",
    "        #now append a row to the dataframe\n",
    "        single_tweet_df = pd.DataFrame([new_entry], columns=cols)\n",
    "        df = df.append(single_tweet_df, ignore_index=True)\n",
    "\n",
    "        #\n",
    "    df['timestamp'] = df.created_at.map(pd.Timestamp)\n",
    "    df = df.sort_values('timestamp')\n",
    "    df = df.drop(['id'],axis=1)  \n",
    "\n",
    "    if not csv_file is None:\n",
    "        #save it to file\n",
    "        df.to_csv(csv_file,mode='a', index=True, encoding=\"utf-8\")\n",
    "            \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4864d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage 1 : read json\n",
    "# stage 2 : get df and implement clean_tweets function\n",
    "# stage 3 : run clean tweets dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2478c151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.conda/lib/python3.8/site-packages/pandas/core/computation/expressions.py:68: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return op(a, b)\n",
      "/workspace/.conda/lib/python3.8/site-packages/pandas/core/computation/expressions.py:68: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return op(a, b)\n"
     ]
    }
   ],
   "source": [
    "_, tweets_list = read_json('covid19.json')\n",
    "# _, insecurity = read_json('insecurity.json')\n",
    "# tweets_list.extend(insecurity)\n",
    "\n",
    "tweets_df = get_tweets_df(csv_file='covid19.csv', json_file=tweets_list)\n",
    "\n",
    "tweets_df = pd.read_csv('covid19.csv')\n",
    "df = clean_tweets_df(tweets_df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "245fcb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3551, 21)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "bc481ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(677, 21)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "871e46ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>source</th>\n",
       "      <th>original_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>lang</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>...</th>\n",
       "      <th>original_author</th>\n",
       "      <th>screen_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>place</th>\n",
       "      <th>place_coord_boundaries</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Fri Jun 18 17:55:49 +0000 2021</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>ðŸš¨Africa is \"in the midst of a full-blown third...</td>\n",
       "      <td>Africa `` midst full-blown third wave '' coron...</td>\n",
       "      <td>Sentiment(polarity=0.0, subjectivity=0.0333333...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>ketuesriche</td>\n",
       "      <td>204051</td>\n",
       "      <td>551</td>\n",
       "      <td>351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TelGlobalHealth, WHOAFRO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mass</td>\n",
       "      <td>2021-06-18 17:55:49+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Fri Jun 18 17:55:59 +0000 2021</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>Dr Moeti is head of WHO in Africa, and one of ...</td>\n",
       "      <td>Dr Moeti head WHO Africa one best public healt...</td>\n",
       "      <td>Sentiment(polarity=0.13333333333333333, subjec...</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Grid1949</td>\n",
       "      <td>3462</td>\n",
       "      <td>66</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>globalhlthtwit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Edinburgh, Scotland</td>\n",
       "      <td>2021-06-18 17:55:59+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Fri Jun 18 17:56:07 +0000 2021</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Thank you @research2note for creating this ama...</td>\n",
       "      <td>Thank creating amazing campaign amp turning so...</td>\n",
       "      <td>Sentiment(polarity=0.3166666666666667, subject...</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>LeeTomlinson8</td>\n",
       "      <td>6727</td>\n",
       "      <td>1195</td>\n",
       "      <td>1176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>red4research</td>\n",
       "      <td>NHSRDForum, Research2note, NHSRDForum</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-06-18 17:56:07+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Fri Jun 18 17:56:10 +0000 2021</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>Former Pfizer VP and Virologist, Dr. Michael Y...</td>\n",
       "      <td>Former Pfizer VP Virologist Dr. Michael Yeadon...</td>\n",
       "      <td>Sentiment(polarity=0.0, subjectivity=0.0)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>RIPNY08</td>\n",
       "      <td>45477</td>\n",
       "      <td>2666</td>\n",
       "      <td>2704</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HighWireTalk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-06-18 17:56:10+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Fri Jun 18 17:56:20 +0000 2021</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>I think itâ€™s important that we donâ€™t sell COVA...</td>\n",
       "      <td>I think important dont sell COVAX short It sti...</td>\n",
       "      <td>Sentiment(polarity=0.225, subjectivity=0.64999...</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>pash22</td>\n",
       "      <td>277957</td>\n",
       "      <td>28250</td>\n",
       "      <td>30819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PeterHotez</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2021-06-18 17:56:20+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                      created_at  \\\n",
       "0           0  Fri Jun 18 17:55:49 +0000 2021   \n",
       "1           1  Fri Jun 18 17:55:59 +0000 2021   \n",
       "2           2  Fri Jun 18 17:56:07 +0000 2021   \n",
       "3           3  Fri Jun 18 17:56:10 +0000 2021   \n",
       "4           4  Fri Jun 18 17:56:20 +0000 2021   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "1  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "3  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "4  <a href=\"http://twitter.com/download/android\" ...   \n",
       "\n",
       "                                       original_text  \\\n",
       "0  ðŸš¨Africa is \"in the midst of a full-blown third...   \n",
       "1  Dr Moeti is head of WHO in Africa, and one of ...   \n",
       "2  Thank you @research2note for creating this ama...   \n",
       "3  Former Pfizer VP and Virologist, Dr. Michael Y...   \n",
       "4  I think itâ€™s important that we donâ€™t sell COVA...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  Africa `` midst full-blown third wave '' coron...   \n",
       "1  Dr Moeti head WHO Africa one best public healt...   \n",
       "2  Thank creating amazing campaign amp turning so...   \n",
       "3  Former Pfizer VP Virologist Dr. Michael Yeadon...   \n",
       "4  I think important dont sell COVAX short It sti...   \n",
       "\n",
       "                                           sentiment  polarity  subjectivity  \\\n",
       "0  Sentiment(polarity=0.0, subjectivity=0.0333333...  0.000000      0.033333   \n",
       "1  Sentiment(polarity=0.13333333333333333, subjec...  0.133333      0.455556   \n",
       "2  Sentiment(polarity=0.3166666666666667, subject...  0.316667      0.483333   \n",
       "3          Sentiment(polarity=0.0, subjectivity=0.0)  0.000000      0.000000   \n",
       "4  Sentiment(polarity=0.225, subjectivity=0.64999...  0.225000      0.650000   \n",
       "\n",
       "  lang  favorite_count  ...  original_author screen_count  followers_count  \\\n",
       "0   en               0  ...      ketuesriche       204051              551   \n",
       "1   en               0  ...         Grid1949         3462               66   \n",
       "2   en               0  ...    LeeTomlinson8         6727             1195   \n",
       "3   en               0  ...          RIPNY08        45477             2666   \n",
       "4   en               0  ...           pash22       277957            28250   \n",
       "\n",
       "   friends_count  possibly_sensitive      hashtags  \\\n",
       "0            351                 NaN           NaN   \n",
       "1             92                 NaN           NaN   \n",
       "2           1176                 NaN  red4research   \n",
       "3           2704                 NaN           NaN   \n",
       "4          30819                 NaN           NaN   \n",
       "\n",
       "                           user_mentions place  place_coord_boundaries  \\\n",
       "0               TelGlobalHealth, WHOAFRO   NaN                    Mass   \n",
       "1                         globalhlthtwit   NaN     Edinburgh, Scotland   \n",
       "2  NHSRDForum, Research2note, NHSRDForum   NaN                     NaN   \n",
       "3                           HighWireTalk   NaN                     NaN   \n",
       "4                             PeterHotez   NaN          United Kingdom   \n",
       "\n",
       "                   timestamp  \n",
       "0  2021-06-18 17:55:49+00:00  \n",
       "1  2021-06-18 17:55:59+00:00  \n",
       "2  2021-06-18 17:56:07+00:00  \n",
       "3  2021-06-18 17:56:10+00:00  \n",
       "4  2021-06-18 17:56:20+00:00  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "1a75b483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>source</th>\n",
       "      <th>original_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>lang</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>...</th>\n",
       "      <th>original_author</th>\n",
       "      <th>screen_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>place</th>\n",
       "      <th>place_coord_boundaries</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Fri Jun 18 17:55:49 +0000 2021</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>ðŸš¨Africa is \"in the midst of a full-blown third...</td>\n",
       "      <td>Africa `` midst full-blown third wave '' coron...</td>\n",
       "      <td>Sentiment(polarity=0.0, subjectivity=0.0333333...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>ketuesriche</td>\n",
       "      <td>204051</td>\n",
       "      <td>551</td>\n",
       "      <td>351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TelGlobalHealth, WHOAFRO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mass</td>\n",
       "      <td>2021-06-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Fri Jun 18 17:55:59 +0000 2021</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>Dr Moeti is head of WHO in Africa, and one of ...</td>\n",
       "      <td>Dr Moeti head WHO Africa one best public healt...</td>\n",
       "      <td>Sentiment(polarity=0.13333333333333333, subjec...</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Grid1949</td>\n",
       "      <td>3462</td>\n",
       "      <td>66</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>globalhlthtwit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Edinburgh, Scotland</td>\n",
       "      <td>2021-06-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Fri Jun 18 17:56:07 +0000 2021</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Thank you @research2note for creating this ama...</td>\n",
       "      <td>Thank creating amazing campaign amp turning so...</td>\n",
       "      <td>Sentiment(polarity=0.3166666666666667, subject...</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>LeeTomlinson8</td>\n",
       "      <td>6727</td>\n",
       "      <td>1195</td>\n",
       "      <td>1176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>red4research</td>\n",
       "      <td>NHSRDForum, Research2note, NHSRDForum</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-06-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Fri Jun 18 17:56:10 +0000 2021</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>Former Pfizer VP and Virologist, Dr. Michael Y...</td>\n",
       "      <td>Former Pfizer VP Virologist Dr. Michael Yeadon...</td>\n",
       "      <td>Sentiment(polarity=0.0, subjectivity=0.0)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>RIPNY08</td>\n",
       "      <td>45477</td>\n",
       "      <td>2666</td>\n",
       "      <td>2704</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HighWireTalk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-06-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Fri Jun 18 17:56:20 +0000 2021</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>I think itâ€™s important that we donâ€™t sell COVA...</td>\n",
       "      <td>I think important dont sell COVAX short It sti...</td>\n",
       "      <td>Sentiment(polarity=0.225, subjectivity=0.64999...</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>pash22</td>\n",
       "      <td>277957</td>\n",
       "      <td>28250</td>\n",
       "      <td>30819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PeterHotez</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2021-06-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                      created_at  \\\n",
       "0           0  Fri Jun 18 17:55:49 +0000 2021   \n",
       "1           1  Fri Jun 18 17:55:59 +0000 2021   \n",
       "2           2  Fri Jun 18 17:56:07 +0000 2021   \n",
       "3           3  Fri Jun 18 17:56:10 +0000 2021   \n",
       "4           4  Fri Jun 18 17:56:20 +0000 2021   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "1  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "3  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "4  <a href=\"http://twitter.com/download/android\" ...   \n",
       "\n",
       "                                       original_text  \\\n",
       "0  ðŸš¨Africa is \"in the midst of a full-blown third...   \n",
       "1  Dr Moeti is head of WHO in Africa, and one of ...   \n",
       "2  Thank you @research2note for creating this ama...   \n",
       "3  Former Pfizer VP and Virologist, Dr. Michael Y...   \n",
       "4  I think itâ€™s important that we donâ€™t sell COVA...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  Africa `` midst full-blown third wave '' coron...   \n",
       "1  Dr Moeti head WHO Africa one best public healt...   \n",
       "2  Thank creating amazing campaign amp turning so...   \n",
       "3  Former Pfizer VP Virologist Dr. Michael Yeadon...   \n",
       "4  I think important dont sell COVAX short It sti...   \n",
       "\n",
       "                                           sentiment  polarity  subjectivity  \\\n",
       "0  Sentiment(polarity=0.0, subjectivity=0.0333333...  0.000000      0.033333   \n",
       "1  Sentiment(polarity=0.13333333333333333, subjec...  0.133333      0.455556   \n",
       "2  Sentiment(polarity=0.3166666666666667, subject...  0.316667      0.483333   \n",
       "3          Sentiment(polarity=0.0, subjectivity=0.0)  0.000000      0.000000   \n",
       "4  Sentiment(polarity=0.225, subjectivity=0.64999...  0.225000      0.650000   \n",
       "\n",
       "  lang  favorite_count  ...  original_author screen_count  followers_count  \\\n",
       "0   en               0  ...      ketuesriche       204051              551   \n",
       "1   en               0  ...         Grid1949         3462               66   \n",
       "2   en               0  ...    LeeTomlinson8         6727             1195   \n",
       "3   en               0  ...          RIPNY08        45477             2666   \n",
       "4   en               0  ...           pash22       277957            28250   \n",
       "\n",
       "   friends_count  possibly_sensitive      hashtags  \\\n",
       "0            351                 NaN           NaN   \n",
       "1             92                 NaN           NaN   \n",
       "2           1176                 NaN  red4research   \n",
       "3           2704                 NaN           NaN   \n",
       "4          30819                 NaN           NaN   \n",
       "\n",
       "                           user_mentions place  place_coord_boundaries  \\\n",
       "0               TelGlobalHealth, WHOAFRO   NaN                    Mass   \n",
       "1                         globalhlthtwit   NaN     Edinburgh, Scotland   \n",
       "2  NHSRDForum, Research2note, NHSRDForum   NaN                     NaN   \n",
       "3                           HighWireTalk   NaN                     NaN   \n",
       "4                             PeterHotez   NaN          United Kingdom   \n",
       "\n",
       "    timestamp  \n",
       "0  2021-06-18  \n",
       "1  2021-06-18  \n",
       "2  2021-06-18  \n",
       "3  2021-06-18  \n",
       "4  2021-06-18  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06873ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clean_Tweets:\n",
    "    \"\"\"\n",
    "    The PEP8 Standard AMAZING!!!\n",
    "    \"\"\"\n",
    "    def __init__(self, df:pd.DataFrame):\n",
    "        self.df = df\n",
    "        print('Automation in Action...!!!')\n",
    "        \n",
    "    def drop_unwanted_column(self, df:pd.DataFrame)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        remove rows that has column names. This error originated from\n",
    "        the data collection stage.  \n",
    "        \"\"\"\n",
    "        unwanted_rows = df[df['retweet_count'] == 'retweet_count' ].index\n",
    "        df.drop(unwanted_rows , inplace=True)\n",
    "        df = df[df['polarity'] != 'polarity']\n",
    "        \n",
    "        return df\n",
    "    def drop_duplicate(self, df:pd.DataFrame)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        drop duplicate rows\n",
    "        \"\"\"\n",
    "        \n",
    "        ---\n",
    "        \n",
    "        return df\n",
    "    def convert_to_datetime(self, df:pd.DataFrame)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        convert column to datetime\n",
    "        \"\"\"\n",
    "        ----\n",
    "        \n",
    "        ----\n",
    "        \n",
    "        df = df[df['created_at'] >= '2020-12-31' ]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def convert_to_numbers(self, df:pd.DataFrame)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        convert columns like polarity, subjectivity, retweet_count\n",
    "        favorite_count etc to numbers\n",
    "        \"\"\"\n",
    "        df['polarity'] = pd.----\n",
    "        \n",
    "        ----\n",
    "        ----\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def remove_non_english_tweets(self, df:pd.DataFrame)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        remove non english tweets from lang\n",
    "        \"\"\"\n",
    "        \n",
    "        df = ----\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534d351e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5a790ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "def read_json(json_file: str)->list:\n",
    "    \"\"\"\n",
    "    json file reader to open and read json files into a list\n",
    "    Args:\n",
    "    -----\n",
    "    json_file: str - path of a json file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    length of the json file and a list of json\n",
    "    \"\"\"\n",
    "    \n",
    "    tweets_data = []\n",
    "    for tweets in open(json_file,'r'):\n",
    "        tweets_data.append(json.loads(tweets))\n",
    "    \n",
    "    \n",
    "    return len(tweets_data), tweets_data\n",
    "    \n",
    "class Tweet_df:\n",
    "    \"\"\"\n",
    "    this function will parse tweets json into a pandas dataframe\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    dataframe\n",
    "    \"\"\"\n",
    "    def __init__(self, tweets_list):\n",
    "        \n",
    "        self.tweets_list = tweets_list\n",
    "\n",
    "    def find_statuses_count(self)->list:\n",
    "        statuses_count = [x['user']['statuses_count'] for x in self.tweets_list]\n",
    "\n",
    "        return statuses_count\n",
    "        \n",
    "    def find_full_text(self)->list:\n",
    "        text = []\n",
    "        for tweet in self.tweets_list:\n",
    "            if 'retweeted_status' in tweet.keys() and 'extended_tweet' in tweet['retweeted_status'].keys():\n",
    "                text.append(tweet['retweeted_status']['extended_tweet']['full_text'])\n",
    "            else: text.append('Empty')\n",
    "\n",
    "        return text       \n",
    "    \n",
    "    def find_sentiments(self, text:list)->list:\n",
    "        polarity, subjectivity = [], []\n",
    "        for tweet in text:\n",
    "            blob = TextBlob(tweet)\n",
    "            sentiment = blob.sentiment\n",
    "            polarity.append(sentiment.polarity)\n",
    "            subjectivity.append(sentiment.subjectivity)\n",
    "\n",
    "        return polarity, subjectivity\n",
    "\n",
    "    def find_created_time(self)->list:\n",
    "        created_at = [x['created_at'] for x in self.tweets_list]\n",
    "\n",
    "        return created_at\n",
    "\n",
    "    def find_source(self)->list:\n",
    "        source = [x['source'] for x in self.tweets_list]\n",
    "\n",
    "        return source\n",
    "\n",
    "    def find_screen_name(self)->list:\n",
    "        screen_name = [x['user']['screen_name'] for x in self.tweets_list]\n",
    "\n",
    "        return screen_name\n",
    "\n",
    "    def find_followers_count(self)->list:\n",
    "        followers_count = [x['user']['followers_count'] for x in self.tweets_list]\n",
    "\n",
    "        return followers_count\n",
    "\n",
    "    def find_friends_count(self)->list:\n",
    "        friends_count = [x['user']['friends_count'] for x in self.tweets_list]\n",
    "\n",
    "        return friends_count\n",
    "\n",
    "    def is_sensitive(self)->list:\n",
    "        is_sensitive = []\n",
    "        for tweet in self.tweets_list:\n",
    "            if 'possibly_sensitive' in tweet.keys():\n",
    "                is_sensitive.append(tweet['possibly_sensitive'])\n",
    "            else: is_sensitive.append(None)\n",
    "    \n",
    "        return is_sensitive\n",
    "       \n",
    "\n",
    "    def find_favourite_count(self)->list:\n",
    "        favorite_count = []\n",
    "        for tweet in self.tweets_list:\n",
    "            if 'retweeted_status' in tweet.keys():\n",
    "                favorite_count.append(tweet['retweeted_status']['favorite_count'])\n",
    "            else: favorite_count.append(0)\n",
    "    \n",
    "        return favorite_count\n",
    "    \n",
    "    def find_retweet_count(self)->list:\n",
    "        \n",
    "        retweet_count = []\n",
    "        for tweet in self.tweets_list:\n",
    "            if 'retweeted_status' in tweet.keys():\n",
    "                retweet_count.append(tweet['retweeted_status']['retweet_count'])\n",
    "            else: retweet_count.append(0)\n",
    "    \n",
    "        return retweet_count\n",
    "\n",
    "    def find_hashtags(self)->list:\n",
    "        hashtags = []\n",
    "        for tw in tweet_list:\n",
    "            hashtags.append(\", \".join([hashtag_item['text'] for hashtag_item in tw['entities']['hashtags']]))\n",
    "\n",
    "        return hashtags\n",
    "\n",
    "    def find_mentions(self)->list:\n",
    "        mentions = []\n",
    "        for tw in tweet_list:\n",
    "            mentions.append( \", \".join([mention['screen_name'] for mention in tw['entities']['user_mentions']]))\n",
    "\n",
    "        return mentions\n",
    "    \n",
    "    def find_lang(self)->list:\n",
    "        lang = [x['lang'] for x in self.tweets_list]\n",
    "        \n",
    "        return lang\n",
    "\n",
    "    def find_location(self)->list:\n",
    "        location = []\n",
    "        for tweet in self.tweets_list:\n",
    "            location.append(tweet['user']['location'])\n",
    "            \n",
    "        return location\n",
    "    \n",
    "    def get_tweet_df(self)->pd.DataFrame:\n",
    "        \"\"\"required column to be generated you should be creative and add more features\"\"\"\n",
    "        \n",
    "        columns = ['created_at', 'source', 'original_text','polarity','subjectivity', 'lang', 'favorite_count', 'retweet_count', \n",
    "            'original_author', 'followers_count','friends_count','possibly_sensitive', 'hashtags', 'user_mentions', 'place']\n",
    "        \n",
    "        created_at = self.find_created_time()\n",
    "        source = self.find_source()\n",
    "        text = self.find_full_text()\n",
    "        polarity, subjectivity = self.find_sentiments(text)\n",
    "        lang = self.find_lang()\n",
    "        fav_count = self.find_favourite_count()\n",
    "        retweet_count = self.find_retweet_count()\n",
    "        screen_name = self.find_screen_name()\n",
    "        follower_count = self.find_followers_count()\n",
    "        friends_count = self.find_friends_count()\n",
    "        sensitivity = self.is_sensitive()\n",
    "        hashtags = self.find_hashtags()\n",
    "        mentions = self.find_mentions()\n",
    "        location = self.find_location()\n",
    "        data = zip(created_at, source, text, polarity, subjectivity, lang, fav_count, retweet_count, screen_name, follower_count, friends_count, sensitivity, hashtags, mentions, location)\n",
    "        df = pd.DataFrame(data=data, columns=columns)\n",
    "        \n",
    "        df.to_csv('processed_tweet_data.csv', index=False)\n",
    "        print('File Successfully Saved.!!!')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "                    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    _, tweet_list = read_json(\"covid19.json\")\n",
    "    \n",
    "    tweet = Tweet_df(tweet_list[:5])\n",
    "#     n_df = tweet.get_tweet_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cc7d6ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mass', 'Edinburgh, Scotland', None, None, 'United Kingdom']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tweet.find_location()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "79c920f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>source</th>\n",
       "      <th>original_text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>lang</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>place_coord_boundaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fri Jun 18 17:55:49 +0000 2021</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>ðŸš¨Africa is \"in the midst of a full-blown third...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.188889</td>\n",
       "      <td>en</td>\n",
       "      <td>548</td>\n",
       "      <td>612</td>\n",
       "      <td>ketuesriche</td>\n",
       "      <td>551</td>\n",
       "      <td>351</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>TelGlobalHealth, WHOAFRO</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fri Jun 18 17:55:59 +0000 2021</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>Dr Moeti is head of WHO in Africa, and one of ...</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>en</td>\n",
       "      <td>195</td>\n",
       "      <td>92</td>\n",
       "      <td>Grid1949</td>\n",
       "      <td>66</td>\n",
       "      <td>92</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>globalhlthtwit</td>\n",
       "      <td>Edinburgh, Scotland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Jun 18 17:56:07 +0000 2021</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Thank you @research2note for creating this ama...</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>en</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>LeeTomlinson8</td>\n",
       "      <td>1195</td>\n",
       "      <td>1176</td>\n",
       "      <td>None</td>\n",
       "      <td>red4research</td>\n",
       "      <td>NHSRDForum, Research2note, NHSRDForum</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fri Jun 18 17:56:10 +0000 2021</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>Former Pfizer VP and Virologist, Dr. Michael Y...</td>\n",
       "      <td>0.086111</td>\n",
       "      <td>0.197222</td>\n",
       "      <td>en</td>\n",
       "      <td>1580</td>\n",
       "      <td>899</td>\n",
       "      <td>RIPNY08</td>\n",
       "      <td>2666</td>\n",
       "      <td>2704</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>HighWireTalk</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fri Jun 18 17:56:20 +0000 2021</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>I think itâ€™s important that we donâ€™t sell COVA...</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>en</td>\n",
       "      <td>72</td>\n",
       "      <td>20</td>\n",
       "      <td>pash22</td>\n",
       "      <td>28250</td>\n",
       "      <td>30819</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>PeterHotez</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at  \\\n",
       "0  Fri Jun 18 17:55:49 +0000 2021   \n",
       "1  Fri Jun 18 17:55:59 +0000 2021   \n",
       "2  Fri Jun 18 17:56:07 +0000 2021   \n",
       "3  Fri Jun 18 17:56:10 +0000 2021   \n",
       "4  Fri Jun 18 17:56:20 +0000 2021   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "1  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "3  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "4  <a href=\"http://twitter.com/download/android\" ...   \n",
       "\n",
       "                                       original_text  polarity  subjectivity  \\\n",
       "0  ðŸš¨Africa is \"in the midst of a full-blown third...  0.166667      0.188889   \n",
       "1  Dr Moeti is head of WHO in Africa, and one of ...  0.133333      0.455556   \n",
       "2  Thank you @research2note for creating this ama...  0.316667      0.483333   \n",
       "3  Former Pfizer VP and Virologist, Dr. Michael Y...  0.086111      0.197222   \n",
       "4  I think itâ€™s important that we donâ€™t sell COVA...  0.280000      0.620000   \n",
       "\n",
       "  lang  favorite_count  retweet_count original_author  followers_count  \\\n",
       "0   en             548            612     ketuesriche              551   \n",
       "1   en             195             92        Grid1949               66   \n",
       "2   en               2              1   LeeTomlinson8             1195   \n",
       "3   en            1580            899         RIPNY08             2666   \n",
       "4   en              72             20          pash22            28250   \n",
       "\n",
       "   friends_count possibly_sensitive      hashtags  \\\n",
       "0            351               None                 \n",
       "1             92               None                 \n",
       "2           1176               None  red4research   \n",
       "3           2704               None                 \n",
       "4          30819               None                 \n",
       "\n",
       "                           user_mentions place_coord_boundaries  \n",
       "0               TelGlobalHealth, WHOAFRO                   Mass  \n",
       "1                         globalhlthtwit    Edinburgh, Scotland  \n",
       "2  NHSRDForum, Research2note, NHSRDForum                   None  \n",
       "3                           HighWireTalk                   None  \n",
       "4                             PeterHotez         United Kingdom  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47b7868c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6532"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "created_at = tweet.find_lang()\n",
    "print(created_at[:10])\n",
    "len(created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab6aa2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = []\n",
    "for tw in tweet_list:\n",
    "    aa.append(\", \".join([hashtag_item['text'] for hashtag_item in tw['entities']['hashtags']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "77d80111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(tweet_list[3]['place'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5440c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d9135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba0fd51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
